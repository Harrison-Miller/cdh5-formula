{%- set hann = salt['mine.get']('G@stack_id:' ~ grains.stack_id ~ ' and G@roles:cdh5.hadoop.namenode.standby', 'grains.items', 'compound') -%}
{%- set namenode_fqdn = salt['mine.get']('G@stack_id:' ~ grains.stack_id ~ ' and G@roles:cdh5.hadoop.namenode', 'grains.items', 'compound').values()[0]['fqdn'] -%}
{%- if hann -%}
  {%- set hann_items = hann.values()[0] if hann else None -%}
  {%- set hann_fqdn = hann_items['fqdn'] if hann else None -%}
  {%- set datanodes = salt['mine.get']('G@stack_id:' ~ grains.stack_id ~ ' and G@roles:cdh5.hadoop.datanode', 'grains.items', 'compound').values()|sort -%}
  {%- set first_dn_fqdn = datanodes[0]['fqdn'] if datanodes else None -%}
  {%- set journal_nodes = [namenode_fqdn ~ ':8485', hann_fqdn ~ ':8485'] -%}
  {%- if first_dn_fqdn -%}
    {%- do journal_nodes.append(first_dn_fqdn ~ ':8485') -%}
  {%- endif -%}
{%- endif -%}
<?xml version="1.0"?>
<configuration>
    <property>
      <name>hadoop.tmp.dir</name>
      <value>/mnt/tmp/hadoop</value>
      <final>true</final>
    </property>
    <property>
      <name>io.file.buffer.size</name>
      <value>65536</value>
    </property>
    <property>
      <name>io.compression.codecs</name>
      <value>org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.SnappyCodec</value>
    </property>
    {%- if hann %}
    <property>
      <name>dfs.nameservices</name>
      <value>default</value>
    </property>
    <property>
      <name>fs.defaultFS</name>
      <value>hdfs://default</value>
    </property>
    <property>
      <name>dfs.ha.namenodes.default</name>
      <value>nn1,nn2</value>
    </property>
    <property>
      <name>dfs.namenode.rpc-address.default.nn1</name>
      <value>{{ namenode_fqdn }}:8020</value>
    </property>
    <property>
      <name>dfs.namenode.rpc-address.default.nn2</name>
      <value>{{ hann_fqdn }}:8020</value>
    </property>
    <property>
      <name>dfs.namenode.http-address.default.nn1</name>
      <value>{{ namenode_fqdn }}:50070</value>
    </property>
    <property>
      <name>dfs.namenode.http-address.default.nn2</name>
      <value>{{ hann_fqdn }}:50070</value>
    </property>
    <property>
      <name>dfs.namenode.shared.edits.dir</name>
      <value>qjournal://{% for j in journal_nodes %}{{ j }}{% if not loop.last %};{% endif %}{% endfor %}/default</value>
    </property>
    <property>
      <name>dfs.journalnode.edits.dir</name>
      <value>{{ pillar.cdh5.dfs.journal_dir }}</value>
    </property>
    <property>
      <name>dfs.client.failover.proxy.provider.mycluster</name>
      <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
    </property>
    <property>
      <name>dfs.ha.fencing.methods</name>
      <value>shell(/bin/true)</value>
    </property>
    {%- else %}
    <property>
      <name>fs.defaultFS</name>
      <value>hdfs://{{ namenode_fqdn }}:8020/</value>
    </property>
    {%- endif %}
    <property>
      <name>fs.trash.interval</name>
      <value>1440</value>
      <final>true</final>
    </property>
    <property>
      <name>fs.checkpoint.dir</name>
      <value>/mnt/hadoop/hdfs/secondary</value>
    </property>
    <property>
      <name>hadoop.rpc.socket.factory.class.default</name>
      <value>org.apache.hadoop.net.StandardSocketFactory</value>
      <final>true</final>
    </property>
    <property>
      <name>hadoop.rpc.socket.factory.class.ClientProtocol</name>
      <value></value>
      <final>true</final>
    </property>
    <property>
      <name>hadoop.proxyuser.oozie.hosts</name>
      <value>*</value>
    </property>
    <property>
      <name>hadoop.proxyuser.oozie.groups</name>
      <value>*</value>
    </property>
    <!-- Hue WebHDFS proxy user setting -->
    <property>
      <name>hadoop.proxyuser.hue.hosts</name>
      <value>*</value>
    </property>
    <property>
      <name>hadoop.proxyuser.hue.groups</name>
      <value>*</value>
    </property>
    <property>
      <name>hadoop.proxyuser.mapred.hosts</name>
      <value>*</value>
    </property>
    <property>
      <name>hadoop.proxyuser.mapred.groups</name>
      <value>*</value>
    </property>
    <property>
      <name>hadoop.proxyuser.hive.hosts</name>
      <value>*</value>
    </property>
    <property>
      <name>hadoop.proxyuser.hive.groups</name>
      <value>*</value>
    </property>
    <property>
       <name>dfs.client.read.shortcircuit</name>
       <value>true</value>
    </property>
{%- if salt['pillar.get']('cdh5:security:enable', False) %}
{%- from 'krb5/settings.sls' import krb5 with context %}
    <property>
      <name>hadoop.security.authentication</name>
      <value>kerberos</value>
    </property>
    <property>
      <name>hadoop.security.authorization</name>
      <value>true</value>
    </property>
    <property>
      <name>hue.kerberos.principal.shortname</name>
      <value>hue</value>
    </property>
{%- endif %}
</configuration>
